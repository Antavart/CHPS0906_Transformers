{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le bon format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Récupérer une image du Dataset ou Dataloader et affichez la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "index = np.random.randint(len(trainset))\n",
    "image, label = trainset[index]\n",
    "\n",
    "image = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Classe: {trainset.classes[label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculer le nombre de patch nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "color_channels = 3\n",
    "patch_size = 4 #votre valeur\n",
    "\n",
    "#Calcul du nombre de patchs\n",
    "number_of_patches = (height * width) // (patch_size**2)\n",
    "print(number_of_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utiliser une convolution pour générer des patchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = color_channels\n",
    "dim_out = patch_size**2\n",
    "patchenizer = nn.Conv2d(dim_in,dim_out,kernel_size=patch_size,stride=patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Créer un objet pour réaliser les patchs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, patch_size):\n",
    "        super(Patch, self).__init__()\n",
    "        self.conv = nn.Conv2d(dim_in, dim_out, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #print(f\"Après convolution: {x.shape}\")  # Debug\n",
    "        x = x.flatten(2)\n",
    "        #print(f\"Après flatten: {x.shape}\")  # Debug\n",
    "        x = x.permute(0, 2, 1)\n",
    "        #print(f\"Après permute: {x.shape}\")  # Debug\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test des dimensions de l'objet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "model = Patch(dim_in,dim_out,patch_size)\n",
    "image, label = next(iter(trainset))\n",
    "image = image.unsqueeze(0)\n",
    "print(image.shape)\n",
    "res = model(image)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de l'attention via SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "        d_k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q,torch.transpose(k, -2, -1))#multiplication matricielle entre q et k\n",
    "        attn_logits = attn_logits/math.sqrt(d_k)#scaling avec d_k (voir equation)\n",
    "        attention = torch.softmax(attn_logits,dim=-1)#faire le softmax sur la dernière dimension\n",
    "        values = torch.matmul(attention,v)#multiplication matricielle avec v\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " torch.Size([3, 2])\n",
      "K\n",
      " torch.Size([3, 2])\n",
      "V\n",
      " torch.Size([3, 2])\n",
      "Values\n",
      " torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe qkv_proj et test avec le scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape: torch.Size([1, 64, 16])\n",
      "k shape: torch.Size([1, 64, 16])\n",
      "v shape: torch.Size([1, 64, 16])\n",
      "Q\n",
      " torch.Size([1, 64, 16])\n",
      "K\n",
      " torch.Size([1, 64, 16])\n",
      "V\n",
      " torch.Size([1, 64, 16])\n",
      "Values\n",
      " torch.Size([1, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "class qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_in, d_out * 3)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        #print(f\"Après proj: {x.shape}\")  # Debug\n",
    "        return x.chunk(3, dim=-1)\n",
    "    \n",
    "model = qkv_proj(dim_out,dim_out)\n",
    "\n",
    "q,k,v = model(res)\n",
    "print(f\"q shape: {q.shape}\")  \n",
    "print(f\"k shape: {k.shape}\")  \n",
    "print(f\"v shape: {v.shape}\")\n",
    "values = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Définition de la classe qkv_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 64, 8]),\n",
       " torch.Size([1, 4, 64, 8]),\n",
       " torch.Size([1, 4, 64, 8]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.proj = nn.Linear(d_in, d_out)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        #print(f\"Batch size : {batch_size}, seq_length : {seq_length}\") # Debug\n",
    "        qkv = self.proj(x)\n",
    "        #print(f\"Après proj: {qkv.shape}\")  # Debug\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, -1)\n",
    "        #print(f\"Après Reshape: {qkv.shape}\")  # Debug\n",
    "        qkv = qkv.permute(0,2,1,3) #permute Batch, Head, SeqLen, Dims\n",
    "        #print(f\"Après permute: {qkv.shape}\")  # Debug\n",
    "        return qkv.chunk(3, dim=-1)\n",
    "    \n",
    "q,k,v = qkv_proj(32, 32*3, 4)(torch.randn(1,64,32))\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        #ps: juste pour vérifier que votre dimension\n",
    "        #   match bien le nombre de tête...\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        #in_proj - votre fonction qkv_proj créé précédement\n",
    "        self.in_proj = qkv_proj(input_dim, embed_dim*3, num_heads)\n",
    "        \n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        q, k, v = self.in_proj(x)\n",
    "        #print(f\"Avant scale: {q.shape} {k.shape} {v.shape} \")  # Debug\n",
    "        #appeler votre fonction scaled_dot_product\n",
    "        attn = scaled_dot_product(q, k, v)\n",
    "        #print(f\"Après scale: {attn.shape}\")  # Debug\n",
    "        #Permute back\n",
    "        #Votre position de départ : [Batch, Head, SeqLen, Dims]\n",
    "        \n",
    "        #Permute dans la nouvelle dim : [Batch, SeqLen, Head, Dims]\n",
    "        #print(f\"Avant permute: {attn.shape}\")  # Debug\n",
    "        attn = attn.permute(0,2,1,3)\n",
    "        #print(f\"Après permute: {attn.shape}\")  # Debug\n",
    "        #reshape pour retirer la dimension \"head\".\n",
    "        #aide : position de départ [Batch, SeqLen, Head, Dims]\n",
    "        # position d'arrivée [Batch, SeqLen, self.embed_dim]\n",
    "        attn = attn.reshape([batch_size,seq_length,self.embed_dim])\n",
    "        out = self.o_proj(attn)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MultiheadAttention(64, 64, 4)\n",
    "net(torch.randn(1,16*16,64)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe FeedFoward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim,hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim,dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe Transformer (Optimus Prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiheadAttention(dim,dim,heads)\n",
    "        self.FFoward = FeedForward(dim,hidden_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.multihead(x)\n",
    "        x = self.FFoward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place de la classe ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe TowerViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TowerViT(nn.Module):\n",
    "    def __init__(self, num_layers, dim, num_heads, hidden_dim, num_classes, patch_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.patch = Patch(3, dim, patch_size)\n",
    "        self.transformer_blocks = nn.Sequential(*[Transformer(dim, num_heads, hidden_dim) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Linear(dim*dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.patch(x)\n",
    "        #print(f\"Après patch: {x.shape}\")  # Debug\n",
    "        x = self.transformer_blocks(x)\n",
    "        #print(f\"Après trans_block: {x.shape}\")  # Debug\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        #print(f\"Après reshape: {x.shape}\")  # Debug\n",
    "        x = self.mlp_head(x)\n",
    "        #print(f\"Après mpl_head: {x.shape}\")  # Debug\n",
    "        #print()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1: Train Loss: 2.1848, Train Acc: 0.1660, Val Loss: 2.0813, Val Acc: 0.2067\n",
      "Epoch 2: Train Loss: 2.0723, Train Acc: 0.2083, Val Loss: 2.0556, Val Acc: 0.2117\n",
      "Epoch 3: Train Loss: 2.0498, Train Acc: 0.2125, Val Loss: 2.0227, Val Acc: 0.2129\n",
      "Epoch 4: Train Loss: 2.0230, Train Acc: 0.2219, Val Loss: 2.0265, Val Acc: 0.2185\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "patch_size = 4\n",
    "num_classes = 10\n",
    "dim = 64\n",
    "num_heads = 4\n",
    "hidden_dim = 128\n",
    "num_layers = 3\n",
    "batch_size = 128\n",
    "learningRate = 0.0001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = TowerViT(num_layers, dim, num_heads, hidden_dim, num_classes, patch_size, batch_size).to(device)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learningRate)   \n",
    " \n",
    "def accuracy(label, res, size=128):\n",
    "  res = (label==res)\n",
    "  \n",
    "  return res.sum()/size\n",
    "\n",
    "def fit_one_cycle(model, train, valid, opt, lossFn, epoch):\n",
    "    model.train()\n",
    "    lossT = 0.0\n",
    "    accT = 0.0\n",
    "    \n",
    "    for batch in train:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        res = model(inputs)\n",
    "        loss = lossFn(res,labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        lossT += loss.item()\n",
    "        _, index= torch.max(res, dim=1)\n",
    "        accT += accuracy(labels, index)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "    \n",
    "    lossT /= len(train)\n",
    "    accT /= len(train)\n",
    "    \n",
    "    model.eval()\n",
    "    lossV = 0.0\n",
    "    accV = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            res = model(inputs)\n",
    "            loss = lossFn(res,labels)\n",
    "            \n",
    "            lossV += loss.item()\n",
    "            _, index= torch.max(res, dim=1)\n",
    "            accV += accuracy(labels, index)\n",
    "        \n",
    "        \n",
    "    lossV /= len(valid)\n",
    "    accV /= len(valid)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {lossT:.4f}, Train Acc: {accT:.4f}, Val Loss: {lossV:.4f}, Val Acc: {accV:.4f}\")\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "train = torch.utils.data.DataLoader(trainset,\n",
    "                                    batch_size=128, \n",
    "                                    shuffle=True\n",
    "                                )\n",
    "valid = torch.utils.data.DataLoader(valset,\n",
    "                                    batch_size=128, \n",
    "                                    shuffle=False\n",
    "                                )\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "for i in range(epoch):\n",
    "    fit_one_cycle(model, train, valid, opt, lossFn, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
