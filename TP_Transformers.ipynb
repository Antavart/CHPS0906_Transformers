{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#%pip install torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "#%pip install einops\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le bon format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Récupérer une image du Dataset ou Dataloader et affichez la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "index = np.random.randint(len(trainset))\n",
    "image, label = trainset[index]\n",
    "\n",
    "image = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Classe: {trainset.classes[label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculer le nombre de patch nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "color_channels = 3\n",
    "patch_size = 4 #votre valeur\n",
    "\n",
    "#Calcul du nombre de patchs\n",
    "number_of_patches = (height * width) // (patch_size**2)\n",
    "print(number_of_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utiliser une convolution pour générer des patchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = color_channels\n",
    "dim_out = patch_size**2\n",
    "patchenizer = nn.Conv2d(dim_in,dim_out,kernel_size=patch_size,stride=patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Créer un objet pour réaliser les patchs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, patch_size):\n",
    "        super(Patch, self).__init__()\n",
    "        self.conv = nn.Conv2d(dim_in, dim_out, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        print(f\"Après convolution: {x.shape}\")  # Debug\n",
    "        x = x.flatten(2)\n",
    "        print(f\"Après flatten: {x.shape}\")  # Debug\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"Après permute: {x.shape}\")  # Debug\n",
    "        return x.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test des dimensions de l'objet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "Après convolution: torch.Size([1, 16, 8, 8])\n",
      "Après flatten: torch.Size([1, 16, 64])\n",
      "Après permute: torch.Size([1, 64, 16])\n",
      "torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "model = Patch(dim_in,dim_out,patch_size)\n",
    "image, label = next(iter(trainset))\n",
    "image = image.unsqueeze(0)\n",
    "print(image.shape)\n",
    "res = model(image)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de l'attention via SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "        d_k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q,torch.transpose(k, 0, 1))#multiplication matricielle entre q et k\n",
    "        attn_logits = attn_logits/math.sqrt(d_k)#scaling avec d_k (voir equation)\n",
    "        attention = torch.softmax(attn_logits,dim=-1)#faire le softmax sur la dernière dimension\n",
    "        values = torch.matmul(attention,v)#multiplication matricielle avec v\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " torch.Size([3, 2])\n",
      "K\n",
      " torch.Size([3, 2])\n",
      "V\n",
      " torch.Size([3, 2])\n",
      "Values\n",
      " torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe qkv_proj et test avec le scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après proj: torch.Size([64, 48])\n",
      "q shape: torch.Size([64, 16])\n",
      "k shape: torch.Size([64, 16])\n",
      "v shape: torch.Size([64, 16])\n",
      "Q\n",
      " torch.Size([64, 16])\n",
      "K\n",
      " torch.Size([64, 16])\n",
      "V\n",
      " torch.Size([64, 16])\n",
      "Values\n",
      " torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "class qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_in, d_out * 3)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        print(f\"Après proj: {x.shape}\")  # Debug\n",
    "        return x.chunk(3, dim=-1)\n",
    "    \n",
    "model = qkv_proj(dim_out,dim_out)\n",
    "\n",
    "q,k,v = model(res)\n",
    "print(f\"q shape: {q.shape}\")  \n",
    "print(f\"k shape: {k.shape}\")  \n",
    "print(f\"v shape: {v.shape}\")\n",
    "values = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Patch.__init__() missing 3 required positional arguments: 'dim_in', 'dim_out', and 'patch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m lossFn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      8\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearningRate)   \n",
      "\u001b[1;31mTypeError\u001b[0m: Patch.__init__() missing 3 required positional arguments: 'dim_in', 'dim_out', and 'patch_size'"
     ]
    }
   ],
   "source": [
    "learningRate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = Patch().to(device)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learningRate)   \n",
    " \n",
    "def accuracy(label, res, size=128):\n",
    "  res = (label==res)\n",
    "  \n",
    "  return res.sum()/size\n",
    "\n",
    "def fit_one_cycle(model, train, valid, opt, lossFn, writer, epoch):\n",
    "    model.train()\n",
    "    lossT = 0.0\n",
    "    accT = 0.0\n",
    "    \n",
    "    for batch in train:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        res = model(inputs)\n",
    "        loss = lossFn(res,labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        lossT += loss.item()\n",
    "        _, index= torch.max(res, dim=1)\n",
    "        accT += accuracy(labels, index)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "    \n",
    "    lossT /= len(train)\n",
    "    accT /= len(train)\n",
    "    \n",
    "    model.eval()\n",
    "    lossV = 0.0\n",
    "    accV = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            res = model(inputs)\n",
    "            loss = lossFn(res,labels)\n",
    "            \n",
    "            lossV += loss.item()\n",
    "            _, index= torch.max(res, dim=1)\n",
    "            accV += accuracy(labels, index)\n",
    "        \n",
    "        \n",
    "    lossV /= len(valid)\n",
    "    accV /= len(valid)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {lossT:.4f}, Train Acc: {accT:.4f}, Val Loss: {lossV:.4f}, Val Acc: {accV:.4f}\")\n",
    "\n",
    "    writer.add_scalars('Loss', {'Train': lossT, 'Validation': lossV}, epoch)\n",
    "    writer.add_scalars('Accuracy', {'Train': accT, 'Validation': accV}, epoch)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "train = torch.utils.data.DataLoader(trainset,\n",
    "                                    batch_size=128, \n",
    "                                    shuffle=True\n",
    "                                )\n",
    "valid = torch.utils.data.DataLoader(valset,\n",
    "                                    batch_size=128, \n",
    "                                    shuffle=False\n",
    "                                )\n",
    "\n",
    "epoch = 10\n",
    "writer = SummaryWriter(log_dir='./logs')\n",
    "for i in range(epoch):\n",
    "    fit_one_cycle(model, train, valid, opt, lossFn, writer, i)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
