{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#%pip install torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "#%pip install einops\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le bon format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Récupérer une image du Dataset ou Dataloader et affichez la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "index = np.random.randint(len(trainset))\n",
    "image, label = trainset[index]\n",
    "\n",
    "image = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Classe: {trainset.classes[label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculer le nombre de patch nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "color_channels = 3\n",
    "patch_size = 4 #votre valeur\n",
    "\n",
    "#Calcul du nombre de patchs\n",
    "number_of_patches = (height * width) // (patch_size**2)\n",
    "print(number_of_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utiliser une convolution pour générer des patchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = color_channels\n",
    "dim_out = patch_size**2\n",
    "patchenizer = nn.Conv2d(dim_in,dim_out,kernel_size=patch_size,stride=patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Créer un objet pour réaliser les patchs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, patch_size):\n",
    "        super(Patch, self).__init__()\n",
    "        self.conv = nn.Conv2d(dim_in, dim_out, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        print(f\"Après convolution: {x.shape}\")  # Debug\n",
    "        x = x.flatten(2)\n",
    "        print(f\"Après flatten: {x.shape}\")  # Debug\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"Après permute: {x.shape}\")  # Debug\n",
    "        return x.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test des dimensions de l'objet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "Après convolution: torch.Size([1, 16, 8, 8])\n",
      "Après flatten: torch.Size([1, 16, 64])\n",
      "Après permute: torch.Size([1, 64, 16])\n",
      "torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "model = Patch(dim_in,dim_out,patch_size)\n",
    "image, label = next(iter(trainset))\n",
    "image = image.unsqueeze(0)\n",
    "print(image.shape)\n",
    "res = model(image)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de l'attention via SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "        d_k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q,torch.transpose(k, -2, -1))#multiplication matricielle entre q et k\n",
    "        attn_logits = attn_logits/math.sqrt(d_k)#scaling avec d_k (voir equation)\n",
    "        attention = torch.softmax(attn_logits,dim=-1)#faire le softmax sur la dernière dimension\n",
    "        values = torch.matmul(attention,v)#multiplication matricielle avec v\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " torch.Size([3, 2])\n",
      "K\n",
      " torch.Size([3, 2])\n",
      "V\n",
      " torch.Size([3, 2])\n",
      "Values\n",
      " torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe qkv_proj et test avec le scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après proj: torch.Size([64, 48])\n",
      "q shape: torch.Size([64, 16])\n",
      "k shape: torch.Size([64, 16])\n",
      "v shape: torch.Size([64, 16])\n",
      "Q\n",
      " torch.Size([64, 16])\n",
      "K\n",
      " torch.Size([64, 16])\n",
      "V\n",
      " torch.Size([64, 16])\n",
      "Values\n",
      " torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "class qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_in, d_out * 3)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        print(f\"Après proj: {x.shape}\")  # Debug\n",
    "        return x.chunk(3, dim=-1)\n",
    "    \n",
    "model = qkv_proj(dim_out,dim_out)\n",
    "\n",
    "q,k,v = model(res)\n",
    "print(f\"q shape: {q.shape}\")  \n",
    "print(f\"k shape: {k.shape}\")  \n",
    "print(f\"v shape: {v.shape}\")\n",
    "values = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Définition de la classe qkv_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 64, 24]),\n",
       " torch.Size([1, 4, 64, 24]),\n",
       " torch.Size([1, 4, 64, 24]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.proj = nn.Linear(d_in, d_out * 3)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        #print(f\"Batch size : {batch_size}, seq_length : {seq_length}\") # Debug\n",
    "        qkv = self.proj(x)\n",
    "        #print(f\"Après proj: {qkv.shape}\")  # Debug\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, -1)\n",
    "        #print(f\"Après Reshape: {qkv.shape}\")  # Debug\n",
    "        qkv = qkv.permute(0,2,1,3) #permute Batch, Head, SeqLen, Dims\n",
    "        #print(f\"Après permute: {qkv.shape}\")  # Debug\n",
    "        return qkv.chunk(3, dim=-1)\n",
    "    \n",
    "q,k,v = qkv_proj(32, 32*3, 4)(torch.randn(1,64,32))\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        #ps: juste pour vérifier que votre dimension\n",
    "        #   match bien le nombre de tête...\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        #in_proj - votre fonction qkv_proj créé précédement\n",
    "        self.in_proj = qkv_proj(input_dim, embed_dim, num_heads)\n",
    "        \n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        q, k, v = self.in_proj(x)\n",
    "        #print(f\"Avant scale: {q.shape} {k.shape} {v.shape} \")  # Debug\n",
    "        #appeler votre fonction scaled_dot_product\n",
    "        attn = scaled_dot_product(q, k, v)\n",
    "        #print(f\"Après scale: {attn.shape}\")  # Debug\n",
    "        #Permute back\n",
    "        #Votre position de départ : [Batch, Head, SeqLen, Dims]\n",
    "        \n",
    "        #Permute dans la nouvelle dim : [Batch, SeqLen, Head, Dims]\n",
    "        #print(f\"Avant permute: {attn.shape}\")  # Debug\n",
    "        attn = attn.permute(0,2,1,3)\n",
    "        #print(f\"Après permute: {attn.shape}\")  # Debug\n",
    "        #reshape pour retirer la dimension \"head\".\n",
    "        #aide : position de départ [Batch, SeqLen, Head, Dims]\n",
    "        # position d'arrivée [Batch, SeqLen, self.embed_dim]\n",
    "        attn = attn.reshape([batch_size,seq_length,self.embed_dim])\n",
    "        out = self.o_proj(attn)\n",
    "\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant permute: torch.Size([1, 4, 256, 16])\n",
      "Après permute: torch.Size([1, 256, 4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MultiheadAttention(64, 64, 4)\n",
    "net(torch.randn(1,16*16,64)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe FeedFoward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim,hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim,dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe Transformer (Optimus Prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiheadAttention(dim,dim,heads)\n",
    "        self.FFoward = FeedForward(dim,hidden_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.multihead(x)\n",
    "        x = self.FFoward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Transformer.__init__() missing 3 required positional arguments: 'dim', 'heads', and 'hidden_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m lossFn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      8\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearningRate)   \n",
      "\u001b[1;31mTypeError\u001b[0m: Transformer.__init__() missing 3 required positional arguments: 'dim', 'heads', and 'hidden_dim'"
     ]
    }
   ],
   "source": [
    "learningRate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = Transformer().to(device)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learningRate)   \n",
    " \n",
    "def accuracy(label, res, size=128):\n",
    "  res = (label==res)\n",
    "  \n",
    "  return res.sum()/size\n",
    "\n",
    "def fit_one_cycle(model, train, valid, opt, lossFn, writer, epoch):\n",
    "    model.train()\n",
    "    lossT = 0.0\n",
    "    accT = 0.0\n",
    "    \n",
    "    for batch in train:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        res = model(inputs)\n",
    "        loss = lossFn(res,labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        lossT += loss.item()\n",
    "        _, index= torch.max(res, dim=1)\n",
    "        accT += accuracy(labels, index)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "    \n",
    "    lossT /= len(train)\n",
    "    accT /= len(train)\n",
    "    \n",
    "    model.eval()\n",
    "    lossV = 0.0\n",
    "    accV = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            res = model(inputs)\n",
    "            loss = lossFn(res,labels)\n",
    "            \n",
    "            lossV += loss.item()\n",
    "            _, index= torch.max(res, dim=1)\n",
    "            accV += accuracy(labels, index)\n",
    "        \n",
    "        \n",
    "    lossV /= len(valid)\n",
    "    accV /= len(valid)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {lossT:.4f}, Train Acc: {accT:.4f}, Val Loss: {lossV:.4f}, Val Acc: {accV:.4f}\")\n",
    "\n",
    "    writer.add_scalars('Loss', {'Train': lossT, 'Validation': lossV}, epoch)\n",
    "    writer.add_scalars('Accuracy', {'Train': accT, 'Validation': accV}, epoch)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "train = torch.utils.data.DataLoader(trainset,\n",
    "                                    batch_size=128, \n",
    "                                    shuffle=True\n",
    "                                )\n",
    "valid = torch.utils.data.DataLoader(valset,\n",
    "                                    batch_size=128, \n",
    "                                    shuffle=False\n",
    "                                )\n",
    "\n",
    "epoch = 10\n",
    "writer = SummaryWriter(log_dir='./logs')\n",
    "for i in range(epoch):\n",
    "    fit_one_cycle(model, train, valid, opt, lossFn, writer, i)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
