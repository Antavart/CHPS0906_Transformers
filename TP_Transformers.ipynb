{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "#%pip install torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "#%pip install einops\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le bon format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Récupérer une image du Dataset ou Dataloader et affichez la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "index = np.random.randint(len(trainset))\n",
    "image, label = trainset[index]\n",
    "\n",
    "image = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Classe: {trainset.classes[label]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculer le nombre de patch nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "color_channels = 3\n",
    "patch_size = 4 #votre valeur\n",
    "\n",
    "#Calcul du nombre de patchs\n",
    "number_of_patches = (height * width) // (patch_size**2)\n",
    "print(number_of_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Utiliser une convolution pour générer des patchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = color_channels\n",
    "dim_out = patch_size**2\n",
    "patchenizer = nn.Conv2d(dim_in,dim_out,kernel_size=patch_size,stride=patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Créer un objet pour réaliser les patchs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, patch_size):\n",
    "        super(Patch, self).__init__()\n",
    "        self.conv = nn.Conv2d(dim_in, dim_out, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #print(f\"Après convolution: {x.shape}\")  # Debug\n",
    "        x = x.flatten(2)\n",
    "        #print(f\"Après flatten: {x.shape}\")  # Debug\n",
    "        x = x.permute(0, 2, 1)\n",
    "        #print(f\"Après permute: {x.shape}\")  # Debug\n",
    "        return x.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test des dimensions de l'objet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "model = Patch(dim_in,dim_out,patch_size)\n",
    "image, label = next(iter(trainset))\n",
    "image = image.unsqueeze(0)\n",
    "print(image.shape)\n",
    "res = model(image)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de l'attention via SoftMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v):\n",
    "        d_k = q.size()[-1]\n",
    "        attn_logits = torch.matmul(q,torch.transpose(k, -2, -1))#multiplication matricielle entre q et k\n",
    "        attn_logits = attn_logits/math.sqrt(d_k)#scaling avec d_k (voir equation)\n",
    "        attention = torch.softmax(attn_logits,dim=-1)#faire le softmax sur la dernière dimension\n",
    "        values = torch.matmul(attention,v)#multiplication matricielle avec v\n",
    "        return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " torch.Size([3, 2])\n",
      "K\n",
      " torch.Size([3, 2])\n",
      "V\n",
      " torch.Size([3, 2])\n",
      "Values\n",
      " torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "values = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe qkv_proj et test avec le scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape: torch.Size([64, 16])\n",
      "k shape: torch.Size([64, 16])\n",
      "v shape: torch.Size([64, 16])\n",
      "Q\n",
      " torch.Size([64, 16])\n",
      "K\n",
      " torch.Size([64, 16])\n",
      "V\n",
      " torch.Size([64, 16])\n",
      "Values\n",
      " torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "class qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_in, d_out * 3)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        #print(f\"Après proj: {x.shape}\")  # Debug\n",
    "        return x.chunk(3, dim=-1)\n",
    "    \n",
    "model = qkv_proj(dim_out,dim_out)\n",
    "\n",
    "q,k,v = model(res)\n",
    "print(f\"q shape: {q.shape}\")  \n",
    "print(f\"k shape: {k.shape}\")  \n",
    "print(f\"v shape: {v.shape}\")\n",
    "values = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q.shape)\n",
    "print(\"K\\n\", k.shape)\n",
    "print(\"V\\n\", v.shape)\n",
    "print(\"Values\\n\", values.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Définition de la classe qkv_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 64, 24]),\n",
       " torch.Size([1, 4, 64, 24]),\n",
       " torch.Size([1, 4, 64, 24]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class qkv_proj(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.proj = nn.Linear(d_in, d_out * 3)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        #print(f\"Batch size : {batch_size}, seq_length : {seq_length}\") # Debug\n",
    "        qkv = self.proj(x)\n",
    "        #print(f\"Après proj: {qkv.shape}\")  # Debug\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, -1)\n",
    "        #print(f\"Après Reshape: {qkv.shape}\")  # Debug\n",
    "        qkv = qkv.permute(0,2,1,3) #permute Batch, Head, SeqLen, Dims\n",
    "        #print(f\"Après permute: {qkv.shape}\")  # Debug\n",
    "        return qkv.chunk(3, dim=-1)\n",
    "    \n",
    "q,k,v = qkv_proj(32, 32*3, 4)(torch.randn(1,64,32))\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        #ps: juste pour vérifier que votre dimension\n",
    "        #   match bien le nombre de tête...\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        #in_proj - votre fonction qkv_proj créé précédement\n",
    "        self.in_proj = qkv_proj(input_dim, embed_dim, num_heads)\n",
    "        \n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        q, k, v = self.in_proj(x)\n",
    "        #print(f\"Avant scale: {q.shape} {k.shape} {v.shape} \")  # Debug\n",
    "        #appeler votre fonction scaled_dot_product\n",
    "        attn = scaled_dot_product(q, k, v)\n",
    "        #print(f\"Après scale: {attn.shape}\")  # Debug\n",
    "        #Permute back\n",
    "        #Votre position de départ : [Batch, Head, SeqLen, Dims]\n",
    "        \n",
    "        #Permute dans la nouvelle dim : [Batch, SeqLen, Head, Dims]\n",
    "        #print(f\"Avant permute: {attn.shape}\")  # Debug\n",
    "        attn = attn.permute(0,2,1,3)\n",
    "        #print(f\"Après permute: {attn.shape}\")  # Debug\n",
    "        #reshape pour retirer la dimension \"head\".\n",
    "        #aide : position de départ [Batch, SeqLen, Head, Dims]\n",
    "        # position d'arrivée [Batch, SeqLen, self.embed_dim]\n",
    "        attn = attn.reshape([batch_size,seq_length,self.embed_dim])\n",
    "        out = self.o_proj(attn)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MultiheadAttention(64, 64, 4)\n",
    "net(torch.randn(1,16*16,64)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe FeedFoward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim,hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim,dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe Transformer (Optimus Prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiheadAttention(dim,dim,heads)\n",
    "        self.FFoward = FeedForward(dim,hidden_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.multihead(x)\n",
    "        x = self.FFoward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en place de la classe ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe TowerViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TowerViT(nn.Module):\n",
    "    def __init__(self, num_layers, dim, num_heads, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = Patch(3, dim, patch_size)\n",
    "        self.transformer_blocks = nn.Sequential(*[Transformer(dim, num_heads, hidden_dim) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.max = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.max(x[:,0])  # Global average pooling\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la classe ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1: Train Loss: 2.3027, Train Acc: 0.0984, Val Loss: 2.3026, Val Acc: 0.0989\n",
      "Epoch 2: Train Loss: 2.3027, Train Acc: 0.0999, Val Loss: 2.3026, Val Acc: 0.0989\n",
      "Epoch 3: Train Loss: 2.3026, Train Acc: 0.0967, Val Loss: 2.3026, Val Acc: 0.0989\n",
      "Epoch 4: Train Loss: 2.3026, Train Acc: 0.0978, Val Loss: 2.3026, Val Acc: 0.0989\n",
      "Epoch 5: Train Loss: 2.3026, Train Acc: 0.0970, Val Loss: 2.3026, Val Acc: 0.0989\n",
      "Epoch 6: Train Loss: 2.3026, Train Acc: 0.0980, Val Loss: 2.3026, Val Acc: 0.0989\n",
      "Epoch 7: Train Loss: 2.3026, Train Acc: 0.0968, Val Loss: 2.3026, Val Acc: 0.0989\n",
      "Epoch 8: Train Loss: 2.3026, Train Acc: 0.0984, Val Loss: 2.3026, Val Acc: 0.0989\n",
      "Epoch 9: Train Loss: 2.3026, Train Acc: 0.0975, Val Loss: 2.3026, Val Acc: 0.0989\n",
      "Epoch 10: Train Loss: 2.3029, Train Acc: 0.0977, Val Loss: 2.3026, Val Acc: 0.0989\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "patch_size = 4\n",
    "num_classes = 10\n",
    "dim = 64\n",
    "num_heads = 4\n",
    "hidden_dim = 128\n",
    "num_layers = 6\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learningRate = 0.001\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = TowerViT(num_layers, dim, num_heads, hidden_dim, num_classes).to(device)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learningRate)   \n",
    " \n",
    "def accuracy(label, res, size=128):\n",
    "  res = (label==res)\n",
    "  \n",
    "  return res.sum()/size\n",
    "\n",
    "def fit_one_cycle(model, train, valid, opt, lossFn, epoch):\n",
    "    model.train()\n",
    "    lossT = 0.0\n",
    "    accT = 0.0\n",
    "    \n",
    "    for batch in train:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        res = model(inputs)\n",
    "        loss = lossFn(res,labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        lossT += loss.item()\n",
    "        _, index= torch.max(res, dim=1)\n",
    "        accT += accuracy(labels, index)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "    \n",
    "    lossT /= len(train)\n",
    "    accT /= len(train)\n",
    "    \n",
    "    model.eval()\n",
    "    lossV = 0.0\n",
    "    accV = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            res = model(inputs)\n",
    "            loss = lossFn(res,labels)\n",
    "            \n",
    "            lossV += loss.item()\n",
    "            _, index= torch.max(res, dim=1)\n",
    "            accV += accuracy(labels, index)\n",
    "        \n",
    "        \n",
    "    lossV /= len(valid)\n",
    "    accV /= len(valid)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {lossT:.4f}, Train Acc: {accT:.4f}, Val Loss: {lossV:.4f}, Val Acc: {accV:.4f}\")\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "valset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "train = torch.utils.data.DataLoader(trainset,\n",
    "                                    batch_size=128, \n",
    "                                    shuffle=True\n",
    "                                )\n",
    "valid = torch.utils.data.DataLoader(valset,\n",
    "                                    batch_size=128, \n",
    "                                    shuffle=False\n",
    "                                )\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "for i in range(epoch):\n",
    "    fit_one_cycle(model, train, valid, opt, lossFn, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
